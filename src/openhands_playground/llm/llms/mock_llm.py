"""Mock LLM implementation for testing and development."""

from typing import Any, Dict, List, Optional

from ..base import BaseLLM


class MockLLM(BaseLLM):
    """Mock LLM implementation that generates predictable responses."""

    def __init__(self, model_name: str = "mock-model", **kwargs: Any) -> None:
        """Initialize the Mock LLM.

        Args:
            model_name: The mock model name
            **kwargs: Additional configuration parameters
        """
        super().__init__(model_name, **kwargs)
        self._responses = [
            "This is a mock response from the LLM.",
            "Here's another simulated AI response.",
            "Mock LLM is working correctly!",
            "This response was generated by the mock implementation.",
            "Testing the LLM factory pattern with mock data.",
        ]

    def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs: Any,
    ) -> str:
        """Generate a mock text response.

        Args:
            prompt: The input prompt (used for deterministic responses)
            max_tokens: Maximum tokens (affects response length simulation)
            temperature: Temperature (affects randomness simulation)
            **kwargs: Additional parameters (ignored)

        Returns:
            Mock generated text response
        """
        # Use prompt hash for deterministic responses in tests
        response_index = hash(prompt) % len(self._responses)
        base_response = self._responses[response_index]

        # Simulate temperature effect
        if temperature and temperature > 0.7:
            base_response += " [High creativity mode]"
        elif temperature and temperature < 0.3:
            base_response += " [Focused mode]"

        # Simulate max_tokens effect
        if max_tokens and max_tokens < 50:
            words = base_response.split()
            truncated_words = words[: max(1, len(words) // 2)]
            base_response = " ".join(truncated_words) + "..."

        return f"[MOCK] {base_response}"

    def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs: Any,
    ) -> str:
        """Generate a mock chat response.

        Args:
            messages: Conversation history
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            **kwargs: Additional parameters (ignored)

        Returns:
            Mock chat response
        """
        if not messages:
            return "[MOCK] Hello! How can I help you today?"

        last_message = messages[-1].get("content", "")

        # Generate contextual mock responses
        if "hello" in last_message.lower() or "hi" in last_message.lower():
            response = "Hello! Nice to meet you. I'm a mock LLM."
        elif "how are you" in last_message.lower():
            response = "I'm doing well, thank you! I'm a mock implementation."
        elif "?" in last_message:
            response = "That's an interesting question! Here's a mock answer."
        else:
            response = (
                f"I understand you said: '{last_message[:50]}...' "
                "Here's my mock response."
            )

        # Apply generation parameters
        if temperature and temperature > 0.8:
            response += " 🎲 [Random mode activated]"

        if max_tokens and max_tokens < 30:
            response = response[:max_tokens] + "..."

        return f"[MOCK] {response}"
